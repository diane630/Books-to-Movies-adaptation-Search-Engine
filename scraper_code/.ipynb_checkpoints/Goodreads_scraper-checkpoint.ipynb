{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re \n",
    "import urllib\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a webdriver object and set options for headless browsing\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(\"./chromedriver.exe\",options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses webdriver object to execute javascript code and get dynamically loaded webcontent\n",
    "def get_js_soup(url,driver):\n",
    "    driver.get(url)\n",
    "    res_html = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
    "    return soup\n",
    "\n",
    "# click load more button on index pages\n",
    "def get_js_soup_loading(url,driver):\n",
    "    driver.get(url)\n",
    "    page_num = 1\n",
    "    if driver.find_elements_by_xpath(\"//span[contains(text(), 'Show more books')]\"):\n",
    "        print(\"loading\", end='')\n",
    "        loadingButton = WebDriverWait(driver,30).until(EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(), 'Show more books')]\")))\n",
    "        maxbooks = 500\n",
    "        while loadingButton:\n",
    "            try:\n",
    "                loadingButton.click()\n",
    "                print(\".\", end='')\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                time.sleep(2)\n",
    "                loadingButton = WebDriverWait(driver,30).until(expected_conditions.visibility_of_element_located((By.XPATH, \"//span[contains(text(), 'Show more books')]\")))\n",
    "                continue\n",
    "            time.sleep(2)\n",
    "            WebDriverWait(driver,30).until_not(EC.presence_of_element_located((By.XPATH,\"//*[contains(text(), 'Loading more books')]\")))\n",
    "            loadElems = driver.find_elements_by_xpath(\"//span[contains(text(), 'Show more books')]\")\n",
    "            if len(loadElems)>0:\n",
    "                loadingButton = WebDriverWait(driver,30).until(EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(), 'Show more books')]\")))\n",
    "                length = len(driver.find_elements_by_css_selector(\".BookListItem__body\"))\n",
    "            else:\n",
    "                break\n",
    "            if length >= maxbooks:\n",
    "                break   \n",
    "    res_html = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
    "    final_length = len(driver.find_elements_by_css_selector(\".BookListItem__body\"))\n",
    "    print(f\"retrieved {final_length} books\")\n",
    "    return soup\n",
    "\n",
    "#tidies extracted text \n",
    "def process_content(content):\n",
    "    content = content.encode('ascii',errors='ignore').decode('utf-8')       #removes non-ascii characters\n",
    "    content = re.sub('\\s+',' ',content)       #repalces repeated whitespace characters with single space\n",
    "    return content\n",
    "\n",
    "''' More tidying\n",
    "Sometimes the text extracted HTML webpage may contain javascript code and some style elements. \n",
    "This function removes script and style tags from HTML so that extracted text does not contain them.\n",
    "'''\n",
    "def remove_script(soup):\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_chars(url):\n",
    "    soup = get_js_soup(url,driver)\n",
    "    chars = []\n",
    "    if soup:\n",
    "        for a in soup.find_all('a', {'href': re.compile('^/characters/.*')}):\n",
    "            chars.append(a.get_text().rstrip().lstrip())\n",
    "    characters = \", \".join(chars)\n",
    "    return characters if characters else 'N/A'\n",
    "\n",
    "def scrape_dir_page(dir_url,driver, i):\n",
    "    soup = get_js_soup_loading(dir_url,driver) \n",
    "    batch = {'url':[], 'title':[], 'story':[], 'writers': [], 'characters':[]}\n",
    "    \n",
    "    for book_holder in soup.find_all('div',class_='BookListItem__body'):\n",
    "        new_url = new_title =  \"\"\n",
    "        new_writers = []\n",
    "        new_characters = \"\"\n",
    "        new_story = \"\"\n",
    "        \n",
    "        # title & url\n",
    "        h3 = book_holder.find('h3', class_='Text Text__title3 Text__umber')\n",
    "        if h3:\n",
    "            strong = h3.find('strong')\n",
    "            if strong:\n",
    "                title_n_url = strong.find('a')\n",
    "                if title_n_url:\n",
    "                    new_title = title_n_url.get_text().rstrip().lstrip()\n",
    "                    new_url = title_n_url['href']\n",
    "                    \n",
    "        # writers\n",
    "        for writer_span in book_holder.find_all('span', class_='ContributorLink__name'):\n",
    "            new_writers.append(writer_span.get_text().rstrip().lstrip())\n",
    "        new_writers = \", \".join(new_writers)\n",
    "            \n",
    "        # story\n",
    "        for story_span in book_holder.find_all('span', class_=\"Formatted\"):\n",
    "            new_story = story_span.get_text().rstrip().lstrip()\n",
    "        \n",
    "        # characters\n",
    "        if new_url:\n",
    "            new_characters = scrape_chars(new_url)\n",
    "        \n",
    "        batch['url'].append(new_url)\n",
    "        batch['title'].append(new_title)\n",
    "        batch['story'].append(new_story)\n",
    "        batch['writers'].append(new_writers)\n",
    "        batch['characters'].append(new_characters)\n",
    "\n",
    "    year = 1921 + last_i - 1\n",
    "    print ('-'*20,'Finish scraping books published in {}'.format(str(year)),'-'*20)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_urls = []\n",
    "book_titles = []\n",
    "book_stories = []\n",
    "book_writers = []\n",
    "book_characters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Scraping directory page --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1935 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1936 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1937 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1938 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1939 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1940 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1941 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1942 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1943 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1944 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1945 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1946 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1947 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1948 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1949 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1950 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1951 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1952 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1953 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1954 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1955 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1956 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1957 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1958 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1959 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1960 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1961 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1962 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1963 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1964 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1965 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1966 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1967 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1968 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1969 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1970 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1971 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1972 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1973 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1974 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1975 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1976 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1977 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1978 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1979 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1980 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1981 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1982 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1983 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1984 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1985 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1986 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1987 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1988 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1989 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1990 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1991 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1992 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1993 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1994 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1995 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1996 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1997 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1998 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 1999 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2000 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2001 --------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2002 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2003 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2004 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2005 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2006 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2007 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2008 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2009 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2010 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2011 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2012 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2013 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2014 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2015 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2016 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2017 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2018 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2019 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2020 --------------------\n",
      "loading.............retrieved 200 books\n",
      "-------------------- Finish scraping books published in 2021 --------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# crawl books published from 1921 to 2021, popularity descending order\n",
    "print ('-'*20,'Scraping directory page','-'*20)\n",
    "\n",
    "# i should start from 1, end at 101 (year 2021)\n",
    "last_i = 15\n",
    "for i in range(last_i, 102):\n",
    "    year = 1921 + last_i - 1\n",
    "    dir_url = 'https://www.goodreads.com/book/popular_by_date/' + str(year)\n",
    "    try:\n",
    "        batch = scrape_dir_page(dir_url,driver, i)\n",
    "        book_urls.extend(batch['url'])\n",
    "        book_titles.extend(batch['title'])\n",
    "        book_stories.extend(batch['story'])\n",
    "        book_writers.extend(batch['writers'])\n",
    "        book_characters.extend(batch['characters'])\n",
    "        last_i += 1\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        batch = scrape_dir_page(dir_url,driver, i)\n",
    "        book_urls.extend(batch['url'])\n",
    "        book_titles.extend(batch['title'])\n",
    "        book_stories.extend(batch['story'])\n",
    "        book_writers.extend(batch['writers'])\n",
    "        book_characters.extend(batch['characters'])\n",
    "        last_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20199"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_urls\n",
    "len(book_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20199"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_titles\n",
    "len(book_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20199"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_stories\n",
    "len(book_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20199"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_writers\n",
    "len(book_writers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20199"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_characters\n",
    "len(book_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lst(lst,file_):\n",
    "    with open(file_,'w') as f:\n",
    "        for l in lst:\n",
    "            f.write(process_content(l))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_urls_file = '../IMDB_goodreads_data/book_urls.txt'\n",
    "book_writers_file = '../IMDB_goodreads_data/book_writers.txt'\n",
    "book_stories_file = '../IMDB_goodreads_data/book_stories.txt'\n",
    "book_titles_file = '../IMDB_goodreads_data/book_titles.txt'\n",
    "book_characters_file = '../IMDB_goodreads_data/book_characters.txt'\n",
    "\n",
    "write_lst(book_urls,book_urls_file)\n",
    "write_lst(book_writers,book_writers_file)\n",
    "write_lst(book_stories,book_stories_file)\n",
    "write_lst(book_titles,book_titles_file)\n",
    "write_lst(book_characters,book_characters_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
